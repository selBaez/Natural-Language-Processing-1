{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "This exercise is meant to help you get familiar with some language data. \n",
    "\n",
    "The corpus used is the **Penn Treebank**, which is a collection of data from the newspaper \n",
    "The Wall Street Journal. The exercise will not take more than a few lines of code; the idea is to examine the data, and notice interesting properties.\n",
    "\n",
    "## Required software\n",
    "\n",
    "### Easy installation\n",
    "\n",
    "The easiest way to get the required software is to install Anaconda. See https://www.continuum.io/downloads . It contains all required packages, including python and jupyter. You can choose python 2.7 or 3.5.\n",
    "\n",
    "### Manual installation\n",
    "\n",
    "Make sure that you have `numpy` and `matplotlib` installed. If you don't, you can use e.g. `pip install <package> --user` (python2) or `pip3 install <package> --user` (python3).\n",
    "\n",
    "## Submission\n",
    "We will grade your assignments with pass/fail/good, so don't forget to hand them in! \n",
    "Choose `File->Download as->HTML` and check if the HTML-file contains all your answers. You can work and submit in pairs. Please e-mail your TA with `\"[NLP1] Assignment 1\"` as the subject. \n",
    "**The deadline for submission is Sunday 6 nov 23:59.**\n",
    "\n",
    "## Start the notebook\n",
    "\n",
    "Start a terminal, and `cd` into the directory where you saved the notebook. Then type `jupyter notebook`. Your web browser will open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with a corpus containing words and their Part-of-speech tags in the format is\n",
    "**word|POS** (one sentence per line) (file name : **sec02-21.gold.tagged**). This data is extracted from Sections 02-21 from the Penn Treebank: these sections are most commonly used for training statistical models (like POS-taggers and parsers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) What are the total number of words (tokens) in this corpus? \n",
    "What is the number of distinct word types?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import operator\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_corpus(data):\n",
    "    n_sentences = 0\n",
    "    \n",
    "    word_pos_combo = []\n",
    "    word_pos_dic = dict()\n",
    "    words = []\n",
    "    pos = []\n",
    "\n",
    "    # Read text from file\n",
    "    with open(data, 'r') as f:\n",
    "        for sentence in f:\n",
    "            n_sentences += 1\n",
    "            temp = sentence.split()\n",
    "            \n",
    "            for i in range(len(temp)):\n",
    "                token = temp[i].split('|')\n",
    "                \n",
    "                word_pos_combo.append(temp[i])\n",
    "                words.append(token[0])\n",
    "                pos.append(token[1])\n",
    "                \n",
    "                if token[0] in word_pos_dic:\n",
    "                    # link word to new POS seen\n",
    "                    if token[1] not in word_pos_dic[token[0]]:\n",
    "                        word_pos_dic[token[0]].append(token[1])\n",
    "                else:\n",
    "                    # create new entry for word\n",
    "                    word_pos_dic[token[0]] = [token[1]]\n",
    "                            \n",
    "            \n",
    "    # Get only distinct words/pos\n",
    "    distinct_word_pos_combo = set(word_pos_combo)\n",
    "    distinct_words = set(words)\n",
    "    distinct_pos = set(pos)\n",
    "    \n",
    "    # Display dataset information\n",
    "    print \"Number of sentences in \", data, \":\", n_sentences\n",
    "    print \"Number of words in \", data, \":\", len(words)\n",
    "    \n",
    "    print \"\\nNumber of distinct words in\", data, \":\", len(distinct_words)\n",
    "    print \"Number of distinct pos in\", data, \":\", len(distinct_pos)\n",
    "    print \"Number of distinct word-pos combinations in \", data, \":\", len(distinct_word_pos_combo)\n",
    "    \n",
    "    return words, pos, word_pos_combo, word_pos_dic, distinct_words, distinct_pos, distinct_word_pos_combo\n",
    "\n",
    "# Proccess training data\n",
    "words, pos, word_pos_combo, word_pos_dic, distinct_words, distinct_pos, distinct_word_pos_combo = read_corpus('sec02-21.gold.tagged')\n",
    "\n",
    "# We need the \"clean\" sentences in order to measure probabilities per sentence\n",
    "clear_sentences = [] \n",
    "clear_text = (' ').join(words)\n",
    "clear_sentences += clear_text.split(' . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print word_pos_dic.keys()[1000], word_pos_dic[word_pos_dic.keys()[1000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Plot a graph of word frequency versus rank of a word, in this corpus. Does this corpus obey Zipfâ€™s law?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_words = Counter(words)\n",
    "sorted_corpus_words = corpus_words.most_common(300)\n",
    "\n",
    "# split words and counts\n",
    "tokens = [x[0] for x in sorted_corpus_words]\n",
    "counts = [x[1] for x in sorted_corpus_words]\n",
    "\n",
    "# create plot\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "# Plot bar with values from dict and label with keys\n",
    "plt.bar(range(len(sorted_corpus_words)), counts, width=0.3)\n",
    "plt.xticks(range(len(sorted_corpus_words)), tokens)\n",
    "\n",
    "# Rotate labels by 90 degrees so you can see them\n",
    "locs, tokens = plt.xticks()\n",
    "plt.setp(tokens, rotation=90,fontsize=6)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to performance issues, we only take the first 3000 words for the plot. Howeveer, it is enough to show that the data follows Zipf's law, with a very long tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) What is the 25th most common word in the corpus? How many times does it occur? What about the 50th most common word, the 100th and the 1000th?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_25 = corpus_words.most_common(25)[-1]\n",
    "most_common_50 = corpus_words.most_common(50)[-1]\n",
    "most_common_100 = corpus_words.most_common(100)[-1]\n",
    "\n",
    "print \"The 25th most common word: \\n\", most_common_25\n",
    "print \"\\nThe 50th most common word: \\n\", most_common_50\n",
    "print \"\\nThe 100th most common word: \\n\", most_common_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) How many different Part-of-speech tags are present in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Number of distinct pos: \", len(distinct_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Print a list of the 10 most common part-of-speech tags. Spend a few minutes trying to guess what each tag means, by looking at associated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_pos = Counter(pos)\n",
    "\n",
    "most_common_10 = corpus_pos.most_common(10)\n",
    "\n",
    "print \"The 10 most common pos: \\n\", most_common_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Assume that the probability $P(w_1^n)$ of a sentence $w_1 \\ldots w_n$   can be calculated as follows:\n",
    "\n",
    "$$P(w_1^n) = P(w_1) \\cdot P(w_2) \\ldots P(w_n) $$\n",
    "\n",
    "The probability of a word $w_i$ can be calculated from a corpus as \n",
    "$$P(w_i) = \\frac{count (w_i)}{N}$$ where $N$ is the total number of word tokens in the corpus. \n",
    "\n",
    "What is the probability of the first two sentences in the corpus? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Function to measure the probability of a uni-gram (w) in a corpus of total count N\n",
    "def get_word_probability(corpus, w):\n",
    "    prob_w = (100.0 * float(corpus[w])) / (100. * float(len(corpus)))\n",
    "    return prob_w\n",
    "\n",
    "\n",
    "#Function to measure the probabilities of each word in a sentence and \n",
    "# the total probability of an n-gram sentence (s) in the corpus\n",
    "def get_sentence_probability(corpus,sentence):\n",
    "    #Probabilities of a sentence\n",
    "    probs_of_s = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        probs_of_s.append(get_word_probability(corpus,word))\n",
    "    \n",
    "    total_prob_s = functools.reduce(operator.mul, probs_of_s, 1)\n",
    "    \n",
    "    return probs_of_s, total_prob_s\n",
    "\n",
    "\n",
    "\n",
    "# Probability of the first two sentences in the corpus\n",
    "enum = 0\n",
    "for sentence in clear_sentences:\n",
    "    if enum < 2:\n",
    "        print \"Sentence is : \", clear_sentences[enum]\n",
    "        print \"Length of sentence is : \", len(clear_sentences[enum].split(\" \")), \" words\"\n",
    "        prob_of_s, total = get_sentence_probability(corpus_words,sentence)\n",
    "        #print \"The probabilities of all words in the sentence is : \", prob_of_s\n",
    "        print \"The total probability of the sentence is : \", total, \"\\n\"\n",
    "        enum += 1\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) A word may have several part-of-speech tags, for example the word 'record' can be a noun or a verb. How many words do have more than one POS tag? What are the 10 most frequent combinations of POS tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Collect words (keys) with multiple pos (values with list longer than 1)\n",
    "words_multiple_pos = []\n",
    "\n",
    "for word, pos in word_pos_dic.items():\n",
    "    if len(pos) > 1:\n",
    "        words_multiple_pos.append(set(pos))\n",
    "        \n",
    "print \"Words with more than one POS: \", len(words_multiple_pos)\n",
    "print \"POS tags combination examples: \", words_multiple_pos[0:10]\n",
    "\n",
    "#frequency_combos = dict()\n",
    "\n",
    "# compare sets and count equals\n",
    "#for i in range(len(words_multiple_pos)):\n",
    "#    for j in range(i+1, len(words_multiple_pos)):\n",
    "#        # find a match\n",
    "#        if words_multiple_pos[i] == words_multiple_pos[j]:\n",
    "#            \n",
    "#            if words_multiple_pos[i] in frequency_combos:\n",
    "#                # increase count\n",
    "#                frequency_combos[words_multiple_pos[i]] += 1\n",
    "#            else:\n",
    "#                # create new entry, start count at 2\n",
    "#                frequency_combos[words_multiple_pos[i]] = 2\n",
    "    \n",
    "    \n",
    "# 10 most frequent word-pos combinations\n",
    "frequency_combos = Counter(word_pos_combo)\n",
    "\n",
    "most_common_10_combo = frequency_combos.most_common(10)\n",
    "\n",
    "print \"\\nThe 10 most common combinations: \\n\", most_common_10_combo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are also provided with another file called **sec00.gold.tagged**. \n",
    "Section 00 of the Penn Treebank is typically used as development data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) How many unseen words are present in the development data (i.e., words that have not occurred in the training data)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Proccess development data\n",
    "words_dev, pos_dev, word_pos_combo_dev, word_pos_dic_dev, distinct_words_dev, distinct_pos_dev, distinct_word_pos_combo_dev = read_corpus('sec00.gold.tagged')\n",
    "\n",
    "\n",
    "# Elements in development data but not in training\n",
    "unseen_words = distinct_words_dev.difference(distinct_words)\n",
    "unseen_pos   = distinct_pos_dev.difference(distinct_pos)\n",
    "\n",
    "print \"\\nNumber of unseen words: \", len(unseen_words), \"\\nFor example: \", list(unseen_words)[0:10]\n",
    "print \"Number of unseen pos: \", len(unseen_pos), \"\\nFor example: \", list(unseen_pos)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) What are the three most common kind of unseen word (their POS tags)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count unseen \n",
    "unseen_word_list = []\n",
    "unseen_pos_list = []\n",
    "\n",
    "for word in words_dev:\n",
    "    if (word in unseen_words):\n",
    "        unseen_word_list.append(word)\n",
    "        \n",
    "\n",
    "for pos in pos_dev:\n",
    "    if (pos in unseen_pos):\n",
    "        unseen_pos_list.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most common unseen elements\n",
    "corpus_word_dev = Counter(unseen_word_list)\n",
    "corpus_pos_dev = Counter(unseen_pos_list)\n",
    "\n",
    "most_common_3_word = corpus_word_dev.most_common(3)\n",
    "most_common_3_pos = corpus_pos_dev.most_common(3)\n",
    "\n",
    "print \"The 3rd most common unseen words: \\n\", most_common_3_word\n",
    "print \"\\nThe 3rd most common unseen pos: \\n\", most_common_3_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even with a large corpus as data, we still have many unseen words with significant count. However, this is not the case for the POS tags, since there is a finite manageable number of tags, it is likely that a large enough corpus will contain all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
